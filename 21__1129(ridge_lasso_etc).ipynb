{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 06. 규제 선형 모델 - 릿지, 라쏘, 엘라스틱넷\n",
    "\n",
    "## 6.2 릿지 회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#보스턴 데이터 세트 로드\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.datasets import load_boston\n",
    "%matplotlib inline\n",
    "\n",
    "boston = load_boston()\n",
    "bostonDF = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "bostonDF['PRICE'] = boston.target\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y_target = bostonDF['PRICE']\n",
    "X_data = bostonDF.drop(['PRICE'], axis=1, inplace=False)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_data, y_target, test_size=0.3, random_state=156)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 fords의 개별 Negative MSE scores: [-11.422 -24.294 -28.144 -74.599 -28.517]\n",
      "5 fords의 개별 RMSE scores: [3.38  4.929 5.305 8.637 5.34 ]\n",
      "5 fords의 평균 RMSE: 5.518\n"
     ]
    }
   ],
   "source": [
    "##릿지 회귀로 보스턴 주택 가격 예측, 예측 성능을 cross_val_score()로 평가\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "#alpha=10으로 설정해 릿지 회귀 수행\n",
    "ridge = Ridge(alpha=10)\n",
    "neg_mse_scores = cross_val_score(ridge, X_data, y_target, scoring='neg_mean_squared_error', cv=5)\n",
    "rmse_scores = np.sqrt(-1*neg_mse_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "\n",
    "print('5 fords의 개별 Negative MSE scores:', np.round(neg_mse_scores, 3))\n",
    "print('5 fords의 개별 RMSE scores:', np.round(rmse_scores, 3))\n",
    "print('5 fords의 평균 RMSE: {0:.3f}'.format(avg_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha 0 일 때 5 folds의 평균 RMSE: 5.829\n",
      "alpha 0.1 일 때 5 folds의 평균 RMSE: 5.788\n",
      "alpha 1 일 때 5 folds의 평균 RMSE: 5.653\n",
      "alpha 10 일 때 5 folds의 평균 RMSE: 5.518\n",
      "alpha 100 일 때 5 folds의 평균 RMSE: 5.330\n"
     ]
    }
   ],
   "source": [
    "##alpha값의 변화에 따른 5폴드의 RMSE 평균값 반환하기\n",
    "\n",
    "#릿지에 사용될 alpha 파라미터 값 정의\n",
    "alphas = [0,0.1,1,10,100]\n",
    "\n",
    "#alphas list 값을 반복하면서 alpha에 따른 평균 rmse를 구함.\n",
    "for alpha in alphas:\n",
    "    ridge = Ridge(alpha = alpha)\n",
    "    #cross_val_score를 이용해 5폴드의 평균 RMSE를 계산\n",
    "    neg_mse_scores = cross_val_score(ridge, X_data, y_target, scoring='neg_mean_squared_error', cv=5)\n",
    "    avg_rmse = np.mean(np.sqrt(-1*neg_mse_scores))\n",
    "    print('alpha {0} 일 때 5 folds의 평균 RMSE: {1:.3f}'.format(alpha, avg_rmse))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1296x432 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#alpha 값의 변화에 따른 피처의 회귀계수값을 가로 막대 그래프로 시각화하기\n",
    "\n",
    "#각 alpha에 따른 회귀계수값을 시각화하기 위해 5개의 열로 된 맷플롯립 축 생성\n",
    "fig, axs = plt.subplots(figsize=(18,6), nrows=1, ncols=5)\n",
    "#각 alpha에 따른 회귀계수값을 데이터로 저장하기 위한 DataFrame 생성\n",
    "coeff_df = pd.DataFrame()\n",
    "\n",
    "#alphas 리스트 값을 차례로 입력해 회귀계수값 시각화 및 데이터 저장. pos는 axis의 위치 지정\n",
    "for pos, alpha in enumerate(alphas):\n",
    "    ridge = Ridge(alpha=alpha)\n",
    "    ridge.fit(X_data, y_target)\n",
    "    #alpha에 따른 피처별로 회귀계수를 Series로 변환하고 이를 DataFrame의 칼럼으로 추가.\n",
    "    coeff = pd.Series(data=ridge.coef_, index=X_data.columns)\n",
    "    colname = 'alpha:'+str(alpha)\n",
    "    coeff_df[colname] = coeff\n",
    "    #막대그래프로 각 alpha값에서의 회귀계수를 시각화. 회귀계수값이 높은 순으로 표현\n",
    "    coeff = coeff.sort_values(ascending=False)\n",
    "    axs[pos].set_title(colname)\n",
    "    axs[pos].set_xlim(-3,6)\n",
    "    sns.barplot(x=coeff.values, y=coeff.index, ax=axs[pos])\n",
    "    \n",
    "#for문 바깥에서 맷플롯립의 show 호출 및 alpha에 따른 피처별 회귀계수를 DataFrame으로 표시\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha:0</th>\n",
       "      <th>alpha:0.1</th>\n",
       "      <th>alpha:1</th>\n",
       "      <th>alpha:10</th>\n",
       "      <th>alpha:100</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RM</th>\n",
       "      <td>3.809865</td>\n",
       "      <td>3.818233</td>\n",
       "      <td>3.854000</td>\n",
       "      <td>3.702272</td>\n",
       "      <td>2.334536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHAS</th>\n",
       "      <td>2.686734</td>\n",
       "      <td>2.670019</td>\n",
       "      <td>2.552393</td>\n",
       "      <td>1.952021</td>\n",
       "      <td>0.638335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAD</th>\n",
       "      <td>0.306049</td>\n",
       "      <td>0.303515</td>\n",
       "      <td>0.290142</td>\n",
       "      <td>0.279596</td>\n",
       "      <td>0.315358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZN</th>\n",
       "      <td>0.046420</td>\n",
       "      <td>0.046572</td>\n",
       "      <td>0.047443</td>\n",
       "      <td>0.049579</td>\n",
       "      <td>0.054496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INDUS</th>\n",
       "      <td>0.020559</td>\n",
       "      <td>0.015999</td>\n",
       "      <td>-0.008805</td>\n",
       "      <td>-0.042962</td>\n",
       "      <td>-0.052826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>0.009312</td>\n",
       "      <td>0.009368</td>\n",
       "      <td>0.009673</td>\n",
       "      <td>0.010037</td>\n",
       "      <td>0.009393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AGE</th>\n",
       "      <td>0.000692</td>\n",
       "      <td>-0.000269</td>\n",
       "      <td>-0.005415</td>\n",
       "      <td>-0.010707</td>\n",
       "      <td>0.001212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAX</th>\n",
       "      <td>-0.012335</td>\n",
       "      <td>-0.012421</td>\n",
       "      <td>-0.012912</td>\n",
       "      <td>-0.013993</td>\n",
       "      <td>-0.015856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRIM</th>\n",
       "      <td>-0.108011</td>\n",
       "      <td>-0.107474</td>\n",
       "      <td>-0.104595</td>\n",
       "      <td>-0.101435</td>\n",
       "      <td>-0.102202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LSTAT</th>\n",
       "      <td>-0.524758</td>\n",
       "      <td>-0.525966</td>\n",
       "      <td>-0.533343</td>\n",
       "      <td>-0.559366</td>\n",
       "      <td>-0.660764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PTRATIO</th>\n",
       "      <td>-0.952747</td>\n",
       "      <td>-0.940759</td>\n",
       "      <td>-0.876074</td>\n",
       "      <td>-0.797945</td>\n",
       "      <td>-0.829218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DIS</th>\n",
       "      <td>-1.475567</td>\n",
       "      <td>-1.459626</td>\n",
       "      <td>-1.372654</td>\n",
       "      <td>-1.248808</td>\n",
       "      <td>-1.153390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOX</th>\n",
       "      <td>-17.766611</td>\n",
       "      <td>-16.684645</td>\n",
       "      <td>-10.777015</td>\n",
       "      <td>-2.371619</td>\n",
       "      <td>-0.262847</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           alpha:0  alpha:0.1    alpha:1  alpha:10  alpha:100\n",
       "RM        3.809865   3.818233   3.854000  3.702272   2.334536\n",
       "CHAS      2.686734   2.670019   2.552393  1.952021   0.638335\n",
       "RAD       0.306049   0.303515   0.290142  0.279596   0.315358\n",
       "ZN        0.046420   0.046572   0.047443  0.049579   0.054496\n",
       "INDUS     0.020559   0.015999  -0.008805 -0.042962  -0.052826\n",
       "B         0.009312   0.009368   0.009673  0.010037   0.009393\n",
       "AGE       0.000692  -0.000269  -0.005415 -0.010707   0.001212\n",
       "TAX      -0.012335  -0.012421  -0.012912 -0.013993  -0.015856\n",
       "CRIM     -0.108011  -0.107474  -0.104595 -0.101435  -0.102202\n",
       "LSTAT    -0.524758  -0.525966  -0.533343 -0.559366  -0.660764\n",
       "PTRATIO  -0.952747  -0.940759  -0.876074 -0.797945  -0.829218\n",
       "DIS      -1.475567  -1.459626  -1.372654 -1.248808  -1.153390\n",
       "NOX     -17.766611 -16.684645 -10.777015 -2.371619  -0.262847"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DataFrame에 저장된 alpha값의 변화에 따른 릿지 회귀계수값 구하기\n",
    "ridge_alphas = [0,0.1,1,10,100]\n",
    "sort_column = 'alpha:'+str(ridge_alphas[0])\n",
    "coeff_df.sort_values(by=sort_column, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 라쏘 회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##alpha값에 따른 폴드 평균 RMSE를 출력하고 회귀계수값들을 DataFrame으로 반환하는 함수 생성하기.\n",
    "\n",
    "from sklearn.linear_model import Lasso, ElasticNet\n",
    "\n",
    "#alpha값에 따른 회귀 모델의 폴드 평균 RMSE를 출력하고 회귀계수값들을 DataFrame으로 반환\n",
    "def get_linear_reg_eval(model_name, params=None, X_data_n=None, y_target_n=None, verbose=True):\n",
    "    coeff_df = pd.DataFrame()\n",
    "    if verbose: print('#######', model_name, '#######')\n",
    "    for param in params:\n",
    "        if model_name == 'Ridge': model=Ridge(alpha=param)\n",
    "        elif model_name == 'Lasso': model=Lasso(alpha=param)\n",
    "        elif model_name == 'ElasticNet': model=ElasticNet(alpha=param, l1_ratio=0.7)\n",
    "        neg_mse_scores = cross_val_score(model, X_data_n, y_target_n, scoring='neg_mean_squared_error', cv=5)\n",
    "        avg_rmse = np.mean(np.sqrt(-1*neg_mse_scores))\n",
    "        print('alpha {0}일 때 5폴드세트의 평균 RMSE: {1:.3f}'.format(param, avg_rmse))\n",
    "        #cross_val_score는 evaluation metric만 반환하므로 모델을 다시 학습하여 회귀 계수 추출\n",
    "        model.fit(X_data, y_target)\n",
    "        #alpha에 따른 피처별 회귀계수를 Series로 변환하고 이를 DataFrame의 칼럼으로 추가\n",
    "        coeff = pd.Series(data=model.coef_, index=X_data.columns)\n",
    "        colname = 'alpha:'+ str(param)\n",
    "        coeff_df[colname] = coeff\n",
    "    return coeff_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### Lasso #######\n",
      "alpha 0.07일 때 5폴드세트의 평균 RMSE: 5.612\n",
      "alpha 0.1일 때 5폴드세트의 평균 RMSE: 5.615\n",
      "alpha 0.5일 때 5폴드세트의 평균 RMSE: 5.669\n",
      "alpha 1일 때 5폴드세트의 평균 RMSE: 5.776\n",
      "alpha 3일 때 5폴드세트의 평균 RMSE: 6.189\n"
     ]
    }
   ],
   "source": [
    "##생성한 함수를 이용해 라쏘 회귀에서의 alpha값의 변화에 따른 RMSE와 그때의 회귀계수들 출력\n",
    "\n",
    "#라쏘에 사용될 alpha 파라미터의 값을 정의하고 get_linear_reg_eval() 함수 호출\n",
    "lasso_alphas = [0.07, 0.1, 0.5, 1, 3]\n",
    "coeff_lasso_df = get_linear_reg_eval('Lasso', params=lasso_alphas, X_data_n=X_data, y_target_n=y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha:0.07</th>\n",
       "      <th>alpha:0.1</th>\n",
       "      <th>alpha:0.5</th>\n",
       "      <th>alpha:1</th>\n",
       "      <th>alpha:3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RM</th>\n",
       "      <td>3.789725</td>\n",
       "      <td>3.703202</td>\n",
       "      <td>2.498212</td>\n",
       "      <td>0.949811</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHAS</th>\n",
       "      <td>1.434343</td>\n",
       "      <td>0.955190</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAD</th>\n",
       "      <td>0.270936</td>\n",
       "      <td>0.274707</td>\n",
       "      <td>0.277451</td>\n",
       "      <td>0.264206</td>\n",
       "      <td>0.061864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZN</th>\n",
       "      <td>0.049059</td>\n",
       "      <td>0.049211</td>\n",
       "      <td>0.049544</td>\n",
       "      <td>0.049165</td>\n",
       "      <td>0.037231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>0.010248</td>\n",
       "      <td>0.010249</td>\n",
       "      <td>0.009469</td>\n",
       "      <td>0.008247</td>\n",
       "      <td>0.006510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOX</th>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AGE</th>\n",
       "      <td>-0.011706</td>\n",
       "      <td>-0.010037</td>\n",
       "      <td>0.003604</td>\n",
       "      <td>0.020910</td>\n",
       "      <td>0.042495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAX</th>\n",
       "      <td>-0.014290</td>\n",
       "      <td>-0.014570</td>\n",
       "      <td>-0.015442</td>\n",
       "      <td>-0.015212</td>\n",
       "      <td>-0.008602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INDUS</th>\n",
       "      <td>-0.042120</td>\n",
       "      <td>-0.036619</td>\n",
       "      <td>-0.005253</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRIM</th>\n",
       "      <td>-0.098193</td>\n",
       "      <td>-0.097894</td>\n",
       "      <td>-0.083289</td>\n",
       "      <td>-0.063437</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LSTAT</th>\n",
       "      <td>-0.560431</td>\n",
       "      <td>-0.568769</td>\n",
       "      <td>-0.656290</td>\n",
       "      <td>-0.761115</td>\n",
       "      <td>-0.807679</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PTRATIO</th>\n",
       "      <td>-0.765107</td>\n",
       "      <td>-0.770654</td>\n",
       "      <td>-0.758752</td>\n",
       "      <td>-0.722966</td>\n",
       "      <td>-0.265072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DIS</th>\n",
       "      <td>-1.176583</td>\n",
       "      <td>-1.160538</td>\n",
       "      <td>-0.936605</td>\n",
       "      <td>-0.668790</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         alpha:0.07  alpha:0.1  alpha:0.5   alpha:1   alpha:3\n",
       "RM         3.789725   3.703202   2.498212  0.949811  0.000000\n",
       "CHAS       1.434343   0.955190   0.000000  0.000000  0.000000\n",
       "RAD        0.270936   0.274707   0.277451  0.264206  0.061864\n",
       "ZN         0.049059   0.049211   0.049544  0.049165  0.037231\n",
       "B          0.010248   0.010249   0.009469  0.008247  0.006510\n",
       "NOX       -0.000000  -0.000000  -0.000000 -0.000000  0.000000\n",
       "AGE       -0.011706  -0.010037   0.003604  0.020910  0.042495\n",
       "TAX       -0.014290  -0.014570  -0.015442 -0.015212 -0.008602\n",
       "INDUS     -0.042120  -0.036619  -0.005253 -0.000000 -0.000000\n",
       "CRIM      -0.098193  -0.097894  -0.083289 -0.063437 -0.000000\n",
       "LSTAT     -0.560431  -0.568769  -0.656290 -0.761115 -0.807679\n",
       "PTRATIO   -0.765107  -0.770654  -0.758752 -0.722966 -0.265072\n",
       "DIS       -1.176583  -1.160538  -0.936605 -0.668790 -0.000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#반환된 coeff_lasso_df를 첫번째 칼럼순으로 내림차순 정렬해 회귀계수 DataFrame 출력\n",
    "sort_column = 'alpha:' + str(lasso_alphas[0])\n",
    "coeff_lasso_df.sort_values(by=sort_column, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 엘라스틱넷 회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### ElasticNet #######\n",
      "alpha 0.07일 때 5폴드세트의 평균 RMSE: 5.542\n",
      "alpha 0.1일 때 5폴드세트의 평균 RMSE: 5.526\n",
      "alpha 0.5일 때 5폴드세트의 평균 RMSE: 5.467\n",
      "alpha 1일 때 5폴드세트의 평균 RMSE: 5.597\n",
      "alpha 3일 때 5폴드세트의 평균 RMSE: 6.068\n"
     ]
    }
   ],
   "source": [
    "##엘라스틱넷 회귀로 엘라스틱넷 alpha값 변화시키면서 RMSE와 각 피처의 회귀계수 출력하기 (위의 함수 이용)\n",
    "\n",
    "#엘라스틱넷에 사용될 alpha 파라미터의 값들을 정의하고 get_linear_reg_eval() 함수 호출\n",
    "#l1_ratio는 0.7로 고정\n",
    "elastic_alphas = [0.07, 0.1, 0.5, 1,3]\n",
    "coeff_elastic_df = get_linear_reg_eval('ElasticNet', params = elastic_alphas, X_data_n=X_data, y_target_n=y_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alpha:0.07</th>\n",
       "      <th>alpha:0.1</th>\n",
       "      <th>alpha:0.5</th>\n",
       "      <th>alpha:1</th>\n",
       "      <th>alpha:3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RM</th>\n",
       "      <td>3.574162</td>\n",
       "      <td>3.414154</td>\n",
       "      <td>1.918419</td>\n",
       "      <td>0.938789</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHAS</th>\n",
       "      <td>1.330724</td>\n",
       "      <td>0.979706</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RAD</th>\n",
       "      <td>0.278880</td>\n",
       "      <td>0.283443</td>\n",
       "      <td>0.300761</td>\n",
       "      <td>0.289299</td>\n",
       "      <td>0.146846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZN</th>\n",
       "      <td>0.050107</td>\n",
       "      <td>0.050617</td>\n",
       "      <td>0.052878</td>\n",
       "      <td>0.052136</td>\n",
       "      <td>0.038268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>B</th>\n",
       "      <td>0.010122</td>\n",
       "      <td>0.010067</td>\n",
       "      <td>0.009114</td>\n",
       "      <td>0.008320</td>\n",
       "      <td>0.007020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AGE</th>\n",
       "      <td>-0.010116</td>\n",
       "      <td>-0.008276</td>\n",
       "      <td>0.007760</td>\n",
       "      <td>0.020348</td>\n",
       "      <td>0.043446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TAX</th>\n",
       "      <td>-0.014522</td>\n",
       "      <td>-0.014814</td>\n",
       "      <td>-0.016046</td>\n",
       "      <td>-0.016218</td>\n",
       "      <td>-0.011417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>INDUS</th>\n",
       "      <td>-0.044855</td>\n",
       "      <td>-0.042719</td>\n",
       "      <td>-0.023252</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CRIM</th>\n",
       "      <td>-0.099468</td>\n",
       "      <td>-0.099213</td>\n",
       "      <td>-0.089070</td>\n",
       "      <td>-0.073577</td>\n",
       "      <td>-0.019058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NOX</th>\n",
       "      <td>-0.175072</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "      <td>-0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LSTAT</th>\n",
       "      <td>-0.574822</td>\n",
       "      <td>-0.587702</td>\n",
       "      <td>-0.693861</td>\n",
       "      <td>-0.760457</td>\n",
       "      <td>-0.800368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PTRATIO</th>\n",
       "      <td>-0.779498</td>\n",
       "      <td>-0.784725</td>\n",
       "      <td>-0.790969</td>\n",
       "      <td>-0.738672</td>\n",
       "      <td>-0.423065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DIS</th>\n",
       "      <td>-1.189438</td>\n",
       "      <td>-1.173647</td>\n",
       "      <td>-0.975902</td>\n",
       "      <td>-0.725174</td>\n",
       "      <td>-0.031208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         alpha:0.07  alpha:0.1  alpha:0.5   alpha:1   alpha:3\n",
       "RM         3.574162   3.414154   1.918419  0.938789  0.000000\n",
       "CHAS       1.330724   0.979706   0.000000  0.000000  0.000000\n",
       "RAD        0.278880   0.283443   0.300761  0.289299  0.146846\n",
       "ZN         0.050107   0.050617   0.052878  0.052136  0.038268\n",
       "B          0.010122   0.010067   0.009114  0.008320  0.007020\n",
       "AGE       -0.010116  -0.008276   0.007760  0.020348  0.043446\n",
       "TAX       -0.014522  -0.014814  -0.016046 -0.016218 -0.011417\n",
       "INDUS     -0.044855  -0.042719  -0.023252 -0.000000 -0.000000\n",
       "CRIM      -0.099468  -0.099213  -0.089070 -0.073577 -0.019058\n",
       "NOX       -0.175072  -0.000000  -0.000000 -0.000000 -0.000000\n",
       "LSTAT     -0.574822  -0.587702  -0.693861 -0.760457 -0.800368\n",
       "PTRATIO   -0.779498  -0.784725  -0.790969 -0.738672 -0.423065\n",
       "DIS       -1.189438  -1.173647  -0.975902 -0.725174 -0.031208"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#반환된 coeff_elastic_ef를 첫번째 칼럼순으로 내림차순 정렬해 회귀계수 DataFrame 출력\n",
    "sort_column = 'alpha:' + str(elastic_alphas[0])\n",
    "coeff_elastic_df.sort_values(by=sort_column, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 선형 회귀 모델을 위한 데이터 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#모듈 불러오기\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "##보스턴의 피처데이터세트에 변환 차례로 적용 후 RMSE로 각 경우별 예측 성능 측정\n",
    "\n",
    "##이를 위해 get_scaled_data() 함수 생성\n",
    "\n",
    "#method는 표준정규분포변환(Standard), 최댓값/최솟값 정규화(MinMax), 로그변환(Log) 결정\n",
    "#p_degree는 다항식 특성을 추가할 때 적용. 2 이상을 부여하지 않음.\n",
    "def get_scaled_data(method='None', p_degree=None, input_data=None):\n",
    "    if method == 'Standard':\n",
    "        scaled_data = StandardScaler().fit_transform(input_data)\n",
    "    elif method == 'MinMax':\n",
    "        scaled_data = MinMaxScaler().fit_transform(input_data)\n",
    "    elif method == 'Log':\n",
    "        scaled_data = np.log1p(input_data)\n",
    "    else:\n",
    "        scaled_data = input_data\n",
    "    \n",
    "    if p_degree != None:\n",
    "        scaled_data = PolynomialFeatures(degree=p_degree, include_bias=False).fit_transform(scaled_data)\n",
    "    \n",
    "    return scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## 변환 유형:None, Polynomial Degree:None\n",
      "alpha 0.1일 때 5폴드세트의 평균 RMSE: 5.788\n",
      "alpha 1일 때 5폴드세트의 평균 RMSE: 5.653\n",
      "alpha 10일 때 5폴드세트의 평균 RMSE: 5.518\n",
      "alpha 100일 때 5폴드세트의 평균 RMSE: 5.330\n",
      "\n",
      "## 변환 유형:Standard, Polynomial Degree:None\n",
      "alpha 0.1일 때 5폴드세트의 평균 RMSE: 5.826\n",
      "alpha 1일 때 5폴드세트의 평균 RMSE: 5.803\n",
      "alpha 10일 때 5폴드세트의 평균 RMSE: 5.637\n",
      "alpha 100일 때 5폴드세트의 평균 RMSE: 5.421\n",
      "\n",
      "## 변환 유형:Standard, Polynomial Degree:2\n",
      "alpha 0.1일 때 5폴드세트의 평균 RMSE: 8.827\n",
      "alpha 1일 때 5폴드세트의 평균 RMSE: 6.871\n",
      "alpha 10일 때 5폴드세트의 평균 RMSE: 5.485\n",
      "alpha 100일 때 5폴드세트의 평균 RMSE: 4.634\n",
      "\n",
      "## 변환 유형:MinMax, Polynomial Degree:None\n",
      "alpha 0.1일 때 5폴드세트의 평균 RMSE: 5.764\n",
      "alpha 1일 때 5폴드세트의 평균 RMSE: 5.465\n",
      "alpha 10일 때 5폴드세트의 평균 RMSE: 5.754\n",
      "alpha 100일 때 5폴드세트의 평균 RMSE: 7.635\n",
      "\n",
      "## 변환 유형:MinMax, Polynomial Degree:2\n",
      "alpha 0.1일 때 5폴드세트의 평균 RMSE: 5.298\n",
      "alpha 1일 때 5폴드세트의 평균 RMSE: 4.323\n",
      "alpha 10일 때 5폴드세트의 평균 RMSE: 5.185\n",
      "alpha 100일 때 5폴드세트의 평균 RMSE: 6.538\n",
      "\n",
      "## 변환 유형:Log, Polynomial Degree:None\n",
      "alpha 0.1일 때 5폴드세트의 평균 RMSE: 4.770\n",
      "alpha 1일 때 5폴드세트의 평균 RMSE: 4.676\n",
      "alpha 10일 때 5폴드세트의 평균 RMSE: 4.836\n",
      "alpha 100일 때 5폴드세트의 평균 RMSE: 6.241\n"
     ]
    }
   ],
   "source": [
    "##Ridge 클래스의 alpha값 변화시키면서 여러 변환한 RMSE값의 변화 살펴보기 (get_linear_reg_eval() 함수 다시 이용)\n",
    "\n",
    "#Ridge의 alpha값을 다르게 적용하고 다양한 데이터 변환 방법에 따른 RMSE 추출\n",
    "alphas = [0.1, 1, 10, 100]\n",
    "\n",
    "#5개 방식으로 변환. 먼저 원본 그대로, 표준정규분포, 표준정규분포+다항식 특성, 최대최소정규화, 최대최소정규화+다항식특성, 로그변환\n",
    "scale_methods = [ (None, None), ('Standard',None), ('Standard',2), ('MinMax',None), ('MinMax',2), ('Log',None) ]\n",
    "for scale_method in scale_methods:\n",
    "    X_data_scaled = get_scaled_data(method=scale_method[0], p_degree=scale_method[1], input_data=X_data)\n",
    "    print('\\n## 변환 유형:{0}, Polynomial Degree:{1}'.format(scale_method[0], scale_method[1]))\n",
    "    get_linear_reg_eval('Ridge', params=alphas, X_data_n=X_data_scaled, y_target_n=y_target, verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07. 로지스틱 회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "##위스콘신 유방암 데이터 세트로 로지스틱회귀 이용 암 여부 판단\n",
    "\n",
    "##데이터 호출\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "##학습/테스트 데이터세트로 나눈 뒤 로지스틱 회귀로 분류 수행\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#StandardScaler()로 평균0, 분산1로 데이터 분포도 변환\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(cancer.data)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_scaled, cancer.target, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.977\n",
      "roc_auc: 0.972\n"
     ]
    }
   ],
   "source": [
    "##로지스틱회귀로 학습 및 예측 수행, 정확도와 roc-auc값 구하기\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "#로지스틱 회귀를 이용해 학습 및 예측 수행\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(X_train, y_train)\n",
    "lr_preds = lr_clf.predict(X_test)\n",
    "\n",
    "#정확도와 roc_auc 측정\n",
    "print('accuracy: {0:.3f}'.format(accuracy_score(y_test,lr_preds)))\n",
    "print('roc_auc: {0:.3f}'.format(roc_auc_score(y_test,lr_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 442, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 442, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 442, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 442, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 442, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최적 하이퍼 파라미터:{'C': 1, 'penalty': 'l2'}, 최적 평균 정확도:0.975\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 531, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1304, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 442, in _check_solver\n",
      "    raise ValueError(\"Solver %s supports only 'l2' or 'none' penalties, \"\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n"
     ]
    }
   ],
   "source": [
    "##GridSearchCv 이용해 위스콘신 데이터 세트에서 penalty, C 하이퍼 파라미터 최적화\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "params = { 'penalty':['l2','l1'], 'C':[0.01,0.1,1,1,5,10] }\n",
    "\n",
    "grid_clf = GridSearchCV(lr_clf, param_grid=params, scoring='accuracy', cv=3)\n",
    "grid_clf.fit(data_scaled, cancer.target)\n",
    "print('최적 하이퍼 파라미터:{0}, 최적 평균 정확도:{1:.3f}'.format(grid_clf.best_params_, grid_clf.best_score_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 08. 회귀 트리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 교차검증의 개별 Negative MSE scores: [ -7.88 -13.14 -20.57 -46.23 -18.88]\n",
      "5 교차검증의 개별 RMSE scores: [2.81 3.63 4.54 6.8  4.34]\n",
      "5 교차검증의 평균 RMSE: 4.423\n"
     ]
    }
   ],
   "source": [
    "##랜덤포레스트 회귀트리인 RandomForestRegressor를 이용해 보스턴 주택 가격 예측 수행하기\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#보스턴 데이터 세트 로드\n",
    "boston = load_boston()\n",
    "bostonDF = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "\n",
    "bostonDF['PRICE'] = boston.target\n",
    "y_target = bostonDF['PRICE']\n",
    "X_data = bostonDF.drop(['PRICE'], axis=1, inplace=False)\n",
    "\n",
    "rf = RandomForestRegressor(random_state=0, n_estimators=1000)\n",
    "neg_mse_scores = cross_val_score(rf, X_data, y_target, scoring='neg_mean_squared_error', cv=5)\n",
    "rmse_scores = np.sqrt(-1*neg_mse_scores)\n",
    "avg_rmse = np.mean(rmse_scores)\n",
    "\n",
    "print('5 교차검증의 개별 Negative MSE scores:', np.round(neg_mse_scores,2))\n",
    "print('5 교차검증의 개별 RMSE scores:', np.round(rmse_scores,2))\n",
    "print('5 교차검증의 평균 RMSE: {0:.3f}'.format(avg_rmse))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
